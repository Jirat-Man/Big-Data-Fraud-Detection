{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7b06c9e-19a6-4d7e-973e-de5e9654805c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a0dd2a7c-1022-4793-b8a4-813e0f93eeb6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#importing all the necessary packages\n",
    "\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import Row\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import re\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.classification import NaiveBayes \n",
    "from pyspark.ml.classification import LinearSVC\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "from pyspark.ml import PipelineModel\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import lit, udf\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from xgboost.spark import SparkXGBRegressor\n",
    "from pyspark.ml.tuning import TrainValidationSplit\n",
    "from pyspark.ml.tuning import TrainValidationSplitModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8458763f-e6ea-4c38-9a2a-43328dcae711",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5e3fe574-cfc6-421d-9ac9-27d939d10a9e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Loading Train Dataset from Azure blob storage container\n",
    "\n",
    "train_transaction = spark.read.format('csv').option(\"header\", 'true').load(\"abfss://team21@sauondbrwebigdatalrs1.dfs.core.windows.net/train_transaction.csv\")\n",
    "train_identity = spark.read.format('csv').option(\"header\", 'true').load(\"abfss://team21@sauondbrwebigdatalrs1.dfs.core.windows.net/train_identity.csv\")\n",
    "\n",
    "# Loading Test Dataset from Azure blob storage container\n",
    "\n",
    "test_transaction = spark.read.format('csv').option(\"header\", 'true').load(\"abfss://team21@sauondbrwebigdatalrs1.dfs.core.windows.net/test_transaction.csv\")\n",
    "test_identity = spark.read.format('csv').option(\"header\", 'true').load(\"abfss://team21@sauondbrwebigdatalrs1.dfs.core.windows.net/test_identity.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cddfc8e2-d870-4161-a7f4-1a4511b092b7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Merging Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3797496d-4347-427a-8bb2-5baef92e4cbc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Using left outer join to get train_df and test_df respectively\n",
    "\n",
    "train_df = train_transaction.join(train_identity,[\"TransactionID\"],\"left\")\n",
    "test_df = test_transaction.join(test_identity,[\"TransactionID\"],\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b0f56129-0576-4a37-b6b8-523a347c5d20",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "###The following columns and their description are taken straight from Kaggle:\n",
    "Reference Link: https://www.kaggle.com/competitions/ieee-fraud-detection/discussion/101203\n",
    "\n",
    "    TransactionDT: timedelta from a given reference datetime (not an actual timestamp)\n",
    "    TransactionAMT: transaction payment amount in USD\n",
    "    ProductCD: product code, the product for each transaction\n",
    "    card1 - card6: payment card information, such as card type, card category, issue bank, country, etc.\n",
    "    addr: address\n",
    "    dist: distance\n",
    "    P_ and (R__) emaildomain: purchaser and recipient email domain\n",
    "    C1-C14: counting, such as how many addresses are found to be associated with the payment card, etc. The actual meaning is masked.\n",
    "    D1-D15: timedelta, such as days between previous transaction, etc.\n",
    "    M1-M9: match, such as names on card and address, etc.\n",
    "    Vxxx: Vesta engineered rich features, including ranking, counting, and other entity relations.\n",
    "    id_1 - id_38: identity information parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64e262f4-5fb9-4d22-a8e8-0f72035f093d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  TransactionID isFraud  ... DeviceType                     DeviceInfo\n0       2987004       0  ...     mobile  SAMSUNG SM-G892A Build/NRD90M\n1       3100441       0  ...     mobile                     iOS Device\n2       3100442       0  ...     mobile                     iOS Device\n3       3100443       0  ...    desktop                    Trident/7.0\n4       3217438       0  ...       None                           None\n\n[5 rows x 434 columns]\n  TransactionID TransactionDT TransactionAmt  ... id-38 DeviceType DeviceInfo\n0       3779617      22364583           49.0  ...  None       None       None\n1       3894291      26465823           54.5  ...  None       None       None\n2       3894292      26466013          226.0  ...  None       None       None\n3       4005646      30142294        1178.66  ...  None       None       None\n4       4005648      30142325           59.0  ...  None       None       None\n\n[5 rows x 433 columns]\n"
     ]
    }
   ],
   "source": [
    "#To show the first 5 elements of the datasets\n",
    "\n",
    "print(train_df.limit(5).toPandas())\n",
    "print(test_df.limit(5).toPandas())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e361d9bb-199c-4b5e-8f33-4507bd52ef05",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Basic Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d6f9e1f-e6a6-43b5-a748-ead79b7c2b10",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "590540\n506691\n"
     ]
    }
   ],
   "source": [
    "#Print Count of Train and Test Dataset\n",
    "\n",
    "print(train_df.count())\n",
    "print(test_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d29cc8ef-80c0-471a-b45b-f1b83cf1f8ed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Print Schema of Train and Test Datasets\n",
    "\n",
    "train_df.printSchema()\n",
    "test_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "baaec297-1509-485a-87a5-7e0328228fbe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#id column names differ in train_df and test_df. Correcting that:\n",
    "\n",
    "test_df = test_df.toDF(*(c.replace('id-', 'id_') for c in test_df.columns)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a34a979-3aed-4bee-994e-a43b695bc59d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Drop duplicate rows from both train and test and caching the datasets\n",
    "\n",
    "train_df = train_df.distinct().cache()\n",
    "test_df = test_df.distinct().cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7b760b5-e192-45eb-86cd-9f07c7fcc53b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c1c7be4-5974-4fb6-a2e0-358f823ea50f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7d2ea74-496d-4fd2-bf23-931245a96a45",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Reference Link: https://medium.com/@mr.priyankmishra/a-realistic-approach-to-ieee-cis-fraud-detection-25faea54137\n",
    "#Calculating missing values in each dataset\n",
    "\n",
    "missing_data_train = train_df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in train_df.columns])\n",
    "missing_data_test = test_df.select([F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in test_df.columns])\n",
    "\n",
    "#Calculating missing values percentage in each dataset\n",
    "\n",
    "missing_value_percentage_train = missing_data_train.select([(F.col(c)/590540).alias(c) for c in train_df.columns]).collect()\n",
    "missing_value_percentage_test = missing_data_test.select([(F.col(c)/506691).alias(c) for c in test_df.columns]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "123906eb-acdb-4998-be82-ffaedd7b670b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Printing the missing values in each column in each dataset\n",
    "\n",
    "missing_data_train.show(2,False,True)\n",
    "missing_data_test.show(2,False,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef3d264d-16a4-4d9f-baa1-bf92d0e775c9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Printing the percentage of missing values in each column in each dataset\n",
    "\n",
    "print(missing_value_percentage_train)\n",
    "print(\"\\n\",missing_value_percentage_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cae24155-9a34-4a65-ba89-576c50ef2ce6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Calculating Distinct Values in each column in each datset\n",
    "#approx_count_distinct function is used to approximately find the no. of distinct values in all 432 fields.\n",
    "\n",
    "distinct_values_train = train_df.select([F.approx_count_distinct(F.col(c)).alias(c) for c in train_df.columns]).collect()\n",
    "distinct_values_test = test_df.select([F.approx_count_distinct(F.col(c)).alias(c) for c in test_df.columns]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81013f65-fa86-45f7-ab69-d53542ae644e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Printing the number of distinct values in each column in each dataset\n",
    "\n",
    "print(distinct_values_train)\n",
    "print(\"\\n\",distinct_values_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "614db457-1bc1-437d-85b8-5facc58434a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Showing imbalance through visualization \n",
    "\n",
    "isFraud_lst = train_df.groupBy('isFraud').count().collect()\n",
    "(x_values, y_values) = zip(*isFraud_lst)\n",
    "plt.bar(x_values, y_values, color = ['red','green'])\n",
    "plt.title('Distribution of Class Labels')\n",
    "plt.xlabel('isFraud')\n",
    "plt.ylabel('Count')\n",
    "plt.text(0,y_values[0]-30000,y_values[0],ha = 'center')\n",
    "plt.text(1,y_values[1]+8000,y_values[1],ha = 'center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "027c4f13-c63a-48f0-9e47-6249dcb408cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Reference Link: https://medium.com/@mr.priyankmishra/a-realistic-approach-to-ieee-cis-fraud-detection-25faea54137\n",
    "#Putting categorical data and numeric data in lists for ease. The list of categorical columns is provided by Kaggle.\n",
    "\n",
    "catf = []\n",
    "catf = ['ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5', \\\n",
    "            'card6', 'addr1', 'addr2', 'P_emaildomain', 'R_emaildomain', 'M1', 'M2', \\\n",
    "            'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9', \\\n",
    "            'DeviceType', 'DeviceInfo']\n",
    "catf+=['id_'+str(i) for i in range(12,39)]\n",
    "print(catf)\n",
    "numf = [feature for feature in train_df.columns if feature not in catf and not feature == 'isFraud']\n",
    "print(numf)\n",
    "numf_plus_label = numf + ['isFraud']\n",
    "\n",
    "#List comprehension is used to seperate numerical from categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "78cf3df7-2b67-44bf-a02c-976f3c2539f3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Casting numeric feature columns and isFraud column to float data type. If the column exists in the numf list, then change its datatype to float.\n",
    "\n",
    "train_df = train_df.select(*(c for c in train_df.columns if c not in numf_plus_label),*(F.col(c).cast(\"float\").alias(c) for c in numf_plus_label))\n",
    "test_df = test_df.select(*(c for c in test_df.columns if c not in numf),*(F.col(c).cast(\"float\").alias(c) for c in numf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "514eed97-3bde-4fa3-9af3-3c0bca646787",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Looking at correlation plot of C columns.\n",
    "#First using regex to search for the column names that are of the form Cxx.\n",
    "#Using vector assembler to prepare the data for correlation.\n",
    "#heatmap is used to plot the data.\n",
    "\n",
    "C_columns = []\n",
    "for ele in train_df.columns:\n",
    "  \n",
    "  if re.search('C\\d+',ele):\n",
    "    C_columns.append(ele)\n",
    "\n",
    "C_columns_df = train_df.select(C_columns)\n",
    "C_columns_assembler = VectorAssembler(inputCols=C_columns_df.columns,outputCol=\"C_columns_vectors\")\n",
    "C_columns_assembler_ouput = C_columns_assembler.setHandleInvalid(\"keep\").transform(C_columns_df)\n",
    "C_columns_matrix = Correlation.corr(C_columns_assembler_ouput, \"C_columns_vectors\")\n",
    "\n",
    "print(\"Pearson correlation matrix:\\n\" + str((C_columns_matrix.head()[0])))\n",
    "\n",
    "x= ['C1','C2','C3','C4','C5','C6','C7','C8','C9','C10','C11','C12', 'C13', 'C14', 'isFraud'] # labels for x-axis\n",
    "y= ['C1','C2','C3','C4','C5','C6','C7','C8','C9','C10','C11','C12', 'C13', 'C14','isFraud'] # labels for y-axis\n",
    "\n",
    "plt.figure(figsize=(16, 7))\n",
    "sns.heatmap(C_columns_matrix.head()[0].toArray(),annot=True, xticklabels=x, yticklabels=y,cmap=\"YlGnBu\", vmin=-1, vmax=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97063f9b-5eaf-44c1-b6d0-0485f08c40eb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Looking at correlation plot of ID columns.\n",
    "#First using regex to search for the column names that are of the form id_xx.\n",
    "#Using vector assembler to prepare the data for correlation.\n",
    "#heatmap is used to plot the data.\n",
    "\n",
    "ID_columns = []\n",
    "for ele in train_df.columns:\n",
    "  \n",
    "  if re.search(r'^id_0[1-9]$|^id_1[0-1]$',ele):\n",
    "    ID_columns.append(ele)\n",
    "\n",
    "ID_columns_df = train_df.select(ID_columns)\n",
    "ID_columns_assembler = VectorAssembler(inputCols=ID_columns_df.columns,outputCol=\"ID_columns_vectors\")\n",
    "ID_columns_assembler_ouput = ID_columns_assembler.setHandleInvalid(\"skip\").transform(ID_columns_df)\n",
    "ID_columns_matrix = Correlation.corr(ID_columns_assembler_ouput, \"ID_columns_vectors\")\n",
    "\n",
    "print(\"Pearson correlation matrix:\\n\" + str((ID_columns_matrix.head()[0])))\n",
    "\n",
    "x= ['id_01', 'id_02', 'id_03', 'id_04', 'id_05', 'id_06', 'id_07', 'id_08', 'id_09', 'id_10', 'id_11'] # labels for x-axis\n",
    "y= ['id_01', 'id_02', 'id_03', 'id_04', 'id_05', 'id_06', 'id_07', 'id_08', 'id_09', 'id_10', 'id_11'] # labels for y-axis\n",
    "\n",
    "plt.figure(figsize=(16, 7))\n",
    "sns.heatmap(ID_columns_matrix.head()[0].toArray(),annot=True, xticklabels=x, yticklabels=y,cmap=\"YlGnBu\", vmin=-1, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc12a987-be8a-412f-aec9-2f06c23326d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Looking at correlation plot of D columns.\n",
    "#First using regex to search for the column names that are of the form Dxx.\n",
    "#Using vector assembler to prepare the data for correlation.\n",
    "#heatmap is used to plot the data.\n",
    "\n",
    "D_columns = []\n",
    "for ele in train_df.columns:\n",
    "  \n",
    "  if re.search(r'D\\d+',ele):\n",
    "    D_columns.append(ele)\n",
    "\n",
    "D_columns_df = train_df.select(D_columns)\n",
    "D_columns_assembler = VectorAssembler(inputCols=D_columns_df.columns,outputCol=\"D_columns_vectors\")\n",
    "D_columns_assembler_ouput = D_columns_assembler.setHandleInvalid(\"keep\").transform(D_columns_df)\n",
    "D_columns_matrix = Correlation.corr(D_columns_assembler_ouput, \"D_columns_vectors\")\n",
    "\n",
    "print(\"Pearson correlation matrix:\\n\" + str((D_columns_matrix.head()[0])))\n",
    "\n",
    "x= ['D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15'] # labels for x-axis\n",
    "y= ['D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D7', 'D8', 'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15'] # labels for y-axis\n",
    "\n",
    "plt.figure(figsize=(16, 7))\n",
    "sns.heatmap(D_columns_matrix.head()[0].toArray(),annot=True, xticklabels=x, yticklabels=y,cmap=\"YlGnBu\", vmin=-1, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d4ef1e8-b659-4cb1-87d7-cb409d46a0c9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Looking at correlation plot of V columns.\n",
    "#First putting all V columns with the same missing values in a 2-D list.\n",
    "#Then correlation plots for each group is plotted after using vector assembler.\n",
    "#In each group's correlation plot, a further subgroup is made of columns with pearson coeff >0.95.\n",
    "#heatmap is used to plot the data.\n",
    "\n",
    "missing_data_train_lst = missing_data_train.select([\"V\"+str(i) for i in range(1,340)]).collect()\n",
    "\n",
    "unique_val = {}\n",
    "for i in range(len(missing_data_train_lst[0])):\n",
    "  if missing_data_train_lst[0][i] not in unique_val:\n",
    "    unique_val[missing_data_train_lst[0][i]] = []\n",
    "  unique_val[missing_data_train_lst[0][i]].append(i+1)\n",
    "\n",
    "V_superset_columns = []\n",
    "for ele in unique_val.values():\n",
    "   V_superset_columns.append([\"V\"+str(i) for i in ele])\n",
    "\n",
    "makegroupings_dict ={}\n",
    "def makegroupings(V_subset_columns_matrix,V_subset_columns):\n",
    "  alreadycomputed = []\n",
    "  for i in range(len(V_subset_columns_matrix)):\n",
    "    if i in alreadycomputed:\n",
    "      continue\n",
    "    else:\n",
    "      makegroupings_dict[V_subset_columns[i]] = []\n",
    "      for j in range(len(V_subset_columns_matrix[0])):\n",
    "        if V_subset_columns_matrix[i][j]>0.75:\n",
    "          makegroupings_dict[V_subset_columns[i]].append(V_subset_columns[j])\n",
    "          alreadycomputed.append(j)\n",
    "  return makegroupings_dict\n",
    "\n",
    "for V_subset_columns in V_superset_columns:\n",
    "  V_subset_columns_df = train_df.select(V_subset_columns)\n",
    "  V_subset_columns_assembler = VectorAssembler(inputCols=V_subset_columns_df.columns,outputCol=\"V_subset_columns_vectors\")\n",
    "  V_subset_columns_assembler_ouput = V_subset_columns_assembler.setHandleInvalid(\"skip\").transform(V_subset_columns_df)\n",
    "  V_subset_columns_matrix = Correlation.corr(V_subset_columns_assembler_ouput, \"V_subset_columns_vectors\")\n",
    "  V_subset_columns_matrix = V_subset_columns_matrix.head()[0].toArray()\n",
    "  \n",
    "  plt.figure(figsize=(20, 20))\n",
    "  sns.heatmap(V_subset_columns_matrix,annot=True, xticklabels=V_subset_columns, yticklabels=V_subset_columns,cmap=\"YlGnBu\", vmin=-1, vmax=1)\n",
    "  plt.show()\n",
    "  makegroupings(V_subset_columns_matrix,V_subset_columns)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73a5b7f9-e4c1-4818-979a-13e0cef7af08",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Replacing null values in categorical columns with 'missing' and in numerical columns with -999\n",
    "\n",
    "numf.remove('TransactionID')\n",
    "train_df = train_df.na.fill(\"missing\",catf).na.fill(-999,numf)\n",
    "test_df  = test_df.na.fill(\"missing\",catf).na.fill(-999,numf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d738c817-845a-4fb6-8e67-6bf9df14be07",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## ML PART 1: RUNNING ON UNPROCESSED DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4eef201-94eb-400a-bf85-436aa30039dc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Pipeline for ML Part 1 is saved and can be loaded as follows (If this cell is used then no need to use the next cell):\n",
    "#pipelineModel = PipelineModel.load(\"abfss://team21@sauondbrwebigdatalrs1.dfs.core.windows.net/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b583b515-69ca-4471-b3af-c0ec6447ada2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#string indexer -> one hot encoding -> vector assembler ->pipeline -> transforming train and test datasets \n",
    "\n",
    "stages = []\n",
    "for catcol in catf:\n",
    "    stringIndexer = StringIndexer(inputCol = catcol, outputCol = catcol + 'Index', handleInvalid = 'keep')\n",
    "    encoder = OneHotEncoder(inputCols=[stringIndexer.getOutputCol()], outputCols=[catcol + \"Vec\"],handleInvalid = 'keep')\n",
    "    stages += [stringIndexer, encoder]\n",
    "\n",
    "assemblerInputs = [c + \"Vec\" for c in catf] + numf\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\",handleInvalid = 'keep')\n",
    "stages += [assembler]\n",
    "\n",
    "pipeline = Pipeline(stages = stages)\n",
    "pipelineModel = pipeline.fit(train_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cb13453-dede-4c7f-adf2-1fdf27f2b8ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#The pipeline model transforms the train and test dataset after being fitted on the train dataset.\n",
    "\n",
    "train_df_base = pipelineModel.transform(train_df)\n",
    "selectedCols = ['features'] + train_df.columns\n",
    "train_df_base = train_df_base.select(selectedCols)\n",
    "\n",
    "test_df_base = pipelineModel.transform(test_df)\n",
    "selectedCols = ['features'] + test_df.columns\n",
    "test_df_base = test_df_base.select(selectedCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ace36092-3d22-409c-8f35-cdc65ec4458f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#To extract second element from each row in the probability column\n",
    "\n",
    "def ith_(v, i):\n",
    "    try:\n",
    "        return float(v[i])\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "ith = udf(ith_, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb77539f-eefa-439e-83a9-b24358159d8b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Logistic Regression model \n",
    "\n",
    "lr = LogisticRegression(featuresCol = 'features', labelCol = 'isFraud', maxIter=10)\n",
    "lrModel = lr.fit(train_df_base.select('isFraud','features'))\n",
    "\n",
    "predictions = lrModel.transform(test_df_base)\n",
    "probability_col_lr = predictions.select(\"TransactionID\",ith(\"probability\", lit(1)))\n",
    "pd_lr = probability_col_lr.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bdb971f-2f94-46ac-9ad5-34f5c27d5ba7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Display the TransactionID and Probability column\n",
    "\n",
    "display(pd_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c79e9c5a-e555-4d47-a216-c605cfc77da2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Gradient Boosted Trees\n",
    "\n",
    "gbt = GBTClassifier(labelCol=\"isFraud\", featuresCol=\"features\", maxIter = 5)\n",
    "gbtModel = gbt.fit(train_df_base.select('isFraud','features'))\n",
    "\n",
    "predictions = gbtModel.transform(test_df_base)\n",
    "probability_col_gbt = predictions.select(\"TransactionID\",ith(\"probability\", lit(1)))\n",
    "pd_gbt = probability_col_gbt.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a669b36d-82a9-45bc-81c8-265c3ff0f430",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Display the TransactionID and Probability column\n",
    "display(pd_gbt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "34a5f4f9-2e36-493a-9c09-625ebfc2f71f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Dropping Features - Pre ML Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc4c1bd1-0c20-4b72-9295-f810650f481f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Reference Link: https://medium.com/@mr.priyankmishra/a-realistic-approach-to-ieee-cis-fraud-detection-25faea54137\n",
    "#Finding columns with more than 90 percent missing values\n",
    "\n",
    "filtered_columns_train = []\n",
    "for i in range(len(missing_value_percentage_train[0])):\n",
    "  if missing_value_percentage_train[0][i]>0.9:\n",
    "    filtered_columns_train.append(train_df.columns[i])\n",
    "\n",
    "filtered_columns_test = []\n",
    "for i in range(len(missing_value_percentage_test[0])):\n",
    "  if missing_value_percentage_test[0][i]>0.9:\n",
    "    filtered_columns_test.append(test_df.columns[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f972cc3b-79d8-4ed5-8f28-4ed528d7b151",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Reference Link: https://medium.com/@mr.priyankmishra/a-realistic-approach-to-ieee-cis-fraud-detection-25faea54137\n",
    "#Finding columns with only a single distinct value\n",
    "\n",
    "filtered_columns_distinct_train = []\n",
    "for i in range(len(distinct_values_train[0])):\n",
    "  if distinct_values_train[0][i]==1:\n",
    "    filtered_columns_distinct_train.append(train_df.columns[i])\n",
    "\n",
    "filtered_columns_distinct_test = []\n",
    "for i in range(len(distinct_values_test[0])):\n",
    "  if distinct_values_test[0][i]==1:\n",
    "    filtered_columns_distinct_test.append(test_df.columns[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c866950c-c101-4e19-983d-a01d2bd29bb6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Once the sub groups are made after plotting the correlation plots, a representative is chosen from each sub group.\n",
    "#This representative has the highest number of distinct values in its subgroup.\n",
    "#The rest of the V columns are dropped.\n",
    "\n",
    "V_distinct_count_dict = {}\n",
    "V_columns_names = [\"V\"+str(i) for i in range(1,340)]\n",
    "V_columns_distinct_count = train_df.select([F.approx_count_distinct(F.col(c)).alias(c) for c in V_columns_names]).collect()\n",
    "\n",
    "for i in range(len(V_columns_names)):\n",
    "  V_distinct_count_dict[V_columns_names[i]] = V_columns_distinct_count[0][i]\n",
    "\n",
    "V_keep = []\n",
    "for value in makegroupings_dict.values():\n",
    "  current_highest_count = 0\n",
    "  \n",
    "  for ele in value:\n",
    "    if V_distinct_count_dict[ele]>current_highest_count:\n",
    "      current_highest_V = ele\n",
    "      current_highest_count = V_distinct_count_dict[ele]\n",
    "  V_keep.append(current_highest_V)\n",
    "\n",
    "V_drop = [c for c in V_columns_names if c not in V_keep]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8f77a46-0bf9-4188-a037-62e73982be15",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Reference Link: https://medium.com/@mr.priyankmishra/a-realistic-approach-to-ieee-cis-fraud-detection-25faea54137\n",
    "#Adding all the columns to be dropped in one list and ensuring that all values are unique by taking set.\n",
    "\n",
    "columns_to_drop = list(set(V_drop + filtered_columns_train + filtered_columns_test + filtered_columns_distinct_train + filtered_columns_distinct_test))\n",
    "print(\"The number of columns that will be dropped are: \",len(columns_to_drop))\n",
    "print(\"\\nThe columns that will be dropped are:\\n\",columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f02daeda-8502-41da-8aee-ec35bf8b076b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Dropping columns by using select statement combined with list comprehension.\n",
    "\n",
    "train_df = train_df.drop(*[F.col(c) for c in columns_to_drop])\n",
    "test_df = test_df.drop(*[F.col(c) for c in columns_to_drop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "177cf331-8b0b-40ff-8afd-30a419b04253",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Reference Link: https://medium.com/@mr.priyankmishra/a-realistic-approach-to-ieee-cis-fraud-detection-25faea54137\n",
    "#Updating list of categorical and numerical features as a lot of features have been dropped.\n",
    "\n",
    "catf = []\n",
    "catf = ['ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5', \\\n",
    "            'card6', 'addr1', 'addr2', 'P_emaildomain', 'R_emaildomain', 'M1', 'M2', \\\n",
    "            'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9', \\\n",
    "            'DeviceType', 'DeviceInfo']\n",
    "catf+=['id_'+str(i) for i in range(12,39)]\n",
    "\n",
    "catf = [f for f in catf if f in train_df.columns]\n",
    "numf = [f for f in train_df.columns if f not in catf and not f == 'isFraud']\n",
    "numf.remove(\"TransactionID\")\n",
    "print(catf)\n",
    "print(numf)\n",
    "numf_plus_label = numf + ['isFraud']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fcdd68e6-fb00-478b-86b6-905dd29734ba",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## ML PART 2: RUNNING ON PREPROCESSED DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "361f2607-e7ec-41d2-9bbc-bbad18dfab04",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#string indexer -> one hot encoding -> vector assembler ->pipeline -> transforming train and test datasets \n",
    "\n",
    "stages2 = []\n",
    "for catcol in catf:\n",
    "    stringIndexer2 = StringIndexer(inputCol = catcol, outputCol = catcol + 'Index', handleInvalid = 'keep')\n",
    "    encoder2 = OneHotEncoder(inputCols=[stringIndexer2.getOutputCol()], outputCols=[catcol + \"Vec\"],handleInvalid = 'keep')\n",
    "    stages2 += [stringIndexer2, encoder2]\n",
    "\n",
    "assemblerInputs = [c + \"Vec\" for c in catf] + numf\n",
    "assembler2 = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\",handleInvalid = 'keep')\n",
    "stages2 += [assembler2]\n",
    "\n",
    "pipeline2 = Pipeline(stages = stages2)\n",
    "pipelineModel2 = pipeline2.fit(train_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09a3241f-7e2f-4677-a686-85b3696f0feb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#The pipeline model transforms the train and test dataset after being fitted on the train dataset.\n",
    "\n",
    "train_df_preprocessed = pipelineModel2.transform(train_df)\n",
    "selectedCols = ['features'] + train_df.columns\n",
    "train_df_preprocessed = train_df_preprocessed.select(selectedCols)\n",
    "\n",
    "test_df_preprocessed = pipelineModel2.transform(test_df)\n",
    "selectedCols = ['features'] + test_df.columns\n",
    "test_df_preprocessed = test_df_preprocessed.select(selectedCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "67588896-f11d-4d88-b30b-2c89982d7262",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#To extract second element from each row in the probability column\n",
    "\n",
    "def ith_(v, i):\n",
    "    try:\n",
    "        return float(v[i])\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "ith = udf(ith_, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fc7a9487-9121-4db3-a66f-1ad50da2e73e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "\n",
    "lr2 = LogisticRegression(featuresCol = 'features', labelCol = 'isFraud', maxIter=10)\n",
    "lrModel2 = lr2.fit(train_df_preprocessed.select('isFraud','features'))\n",
    "\n",
    "predictions = lrModel2.transform(test_df_preprocessed)\n",
    "probability_col_lr2 = predictions.select(\"TransactionID\",ith(\"probability\", lit(1)))\n",
    "\n",
    "pd_lr2 = probability_col_lr2.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e3ab685-ef20-4461-bbee-5e70029ac7b2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Display the TransactionID and Probability column\n",
    "display(pd_lr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f07c344f-5b47-41be-943f-b8e92d017143",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Gradient Boosted Trees\n",
    "\n",
    "gbt2 = GBTClassifier(labelCol=\"isFraud\", featuresCol=\"features\", maxIter = 5)\n",
    "gbtModel2 = gbt2.fit(train_df_preprocessed.select('isFraud','features'))\n",
    "\n",
    "predictions = gbtModel2.transform(test_df_preprocessed)\n",
    "probability_col_gbt2 = predictions.select(\"TransactionID\",ith(\"probability\", lit(1)))\n",
    "\n",
    "pd_gbt2 = probability_col_gbt2.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c048b09-a66f-4f99-b5b3-5c3b1cc0f549",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Display the TransactionID and Probability column\n",
    "display(pd_gbt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dfb7bc2d-af8e-49b2-b7b2-295cef1ae624",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Adding Features - Pre ML Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a118e759-36b5-4267-98e7-2ddd4dc8706d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Replacing \"missing\" and -999 with None for this step.\n",
    "#We are doing this because we will be engineering features from existing columns in the next steps and we want to have None in those columns wherever applicable.\n",
    "\n",
    "train_df = train_df.select([F.when(F.col(c)==\"missing\",None).when(F.col(c)==-999,None).otherwise(F.col(c)).alias(c) for c in train_df.columns])\n",
    "\n",
    "test_df = test_df.select([F.when(F.col(c)==\"missing\",None).when(F.col(c)==-999,None).otherwise(F.col(c)).alias(c) for c in test_df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62497386-278f-46cb-aacc-73322952fe9b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Reference Link: https://medium.com/@mr.priyankmishra/a-realistic-approach-to-ieee-cis-fraud-detection-25faea54137\n",
    "#Reference Link: https://sparkbyexamples.com/pyspark/pyspark-add-new-column-to-dataframe/#add-column-based-on-condition\n",
    "\n",
    "# adding transaction hour column by using the existing TransactionDT column.\n",
    "train_df = train_df.withColumn(\"New_Transaction_Hour\", F.floor(F.col('TransactionDT')/3600)%24)\n",
    "test_df = test_df.withColumn(\"New_Transaction_Hour\", F.floor(F.col('TransactionDT')/3600)%24)\n",
    "\n",
    "# adding uid (unique identifier) column (combination of card1, card2, card3, card4, card5, card6, addr1 and P_emaildomain)\n",
    "train_df= train_df.withColumn(\"uid\", F.concat(train_df[\"card1\"],train_df[\"card2\"],train_df[\"card3\"],train_df[\"card4\"],train_df[\"card5\"],train_df[\"card6\"], train_df[\"addr1\"],train_df[\"P_emaildomain\"]))\n",
    "\n",
    "test_df = test_df.withColumn(\"uid\", F.concat(test_df[\"card1\"],test_df[\"card2\"],test_df[\"card3\"],test_df[\"card4\"],test_df[\"card5\"],test_df[\"card6\"], test_df[\"addr1\"],test_df[\"P_emaildomain\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bb7b3b3d-b98b-4867-9a4d-f126f7276553",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Reference Link: https://medium.com/@mr.priyankmishra/a-realistic-approach-to-ieee-cis-fraud-detection-25faea54137\n",
    "#Updating list of categorical and numerical features \n",
    "\n",
    "catf = []\n",
    "catf = ['ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5', \\\n",
    "            'card6', 'addr1', 'addr2', 'P_emaildomain', 'R_emaildomain', 'M1', 'M2', \\\n",
    "            'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9', \\\n",
    "            'DeviceType', 'DeviceInfo','uid']\n",
    "catf+=['id_'+str(i) for i in range(12,39)]\n",
    "\n",
    "catf = [f for f in catf if f in train_df.columns]\n",
    "numf = [f for f in train_df.columns if f not in catf and not f == 'isFraud']\n",
    "numf.remove(\"TransactionID\")\n",
    "print(catf)\n",
    "print(numf)\n",
    "numf_plus_label = numf + ['isFraud']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1c10f2d-189b-412c-a19d-b95106289a42",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Re-imputing missing values as feature engineering is done \n",
    "\n",
    "train_df = train_df.na.fill(\"missing\",catf).na.fill(-999,numf)\n",
    "test_df  = test_df.na.fill(\"missing\",catf).na.fill(-999,numf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "323a60e3-dd40-43cb-9049-4224af4b8f67",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62503d92-ae9b-40f3-a074-2e2447ffd59d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df = train_df.unpersist()\n",
    "test_df = test_df.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d1d2810-b24a-4833-ad36-012c02798b0e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df = train_df.cache()\n",
    "test_df = test_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56038feb-504f-41c2-bc27-f0c9532c92fd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcfa8e0c-4057-45fd-8377-a52d2b930f34",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "test_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e417630-5850-4adc-834f-7a3d993d46c4",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## ML PART 3: RUNNING AFTER FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b88136d7-e882-4576-9fce-b776b368dd5f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#string indexer -> one hot encoding -> vector assembler ->pipeline -> transforming train and test datasets \n",
    "\n",
    "stages3 = []\n",
    "for catcol in catf:\n",
    "    stringIndexer3 = StringIndexer(inputCol = catcol, outputCol = catcol + 'Index', handleInvalid = 'keep')\n",
    "    encoder3 = OneHotEncoder(inputCols=[stringIndexer3.getOutputCol()], outputCols=[catcol + \"Vec\"],handleInvalid = 'keep')\n",
    "    stages3 += [stringIndexer3, encoder3]\n",
    "  \n",
    "assemblerInputs = [c + \"Vec\" for c in catf] + numf\n",
    "assembler3 = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\",handleInvalid = 'keep')\n",
    "stages3 += [assembler3]\n",
    "\n",
    "pipeline3 = Pipeline(stages = stages3)\n",
    "pipelineModel3 = pipeline3.fit(train_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "937975bf-3ac9-45c6-ac1e-d180981dff7c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#The pipeline model transforms the train and test dataset after being fitted on the train dataset.\n",
    "\n",
    "train_df_feature = pipelineModel3.transform(train_df)\n",
    "selectedCols = ['features'] + train_df.columns\n",
    "train_df_feature = train_df_feature.select(selectedCols)\n",
    "\n",
    "test_df_feature = pipelineModel3.transform(test_df)\n",
    "selectedCols = ['features'] + test_df.columns\n",
    "test_df_feature = test_df_feature.select(selectedCols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e76349f-3208-4ec8-8036-4ca85b247583",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#To extract second element from each row in the probability column\n",
    "\n",
    "def ith_(v, i):\n",
    "    try:\n",
    "        return float(v[i])\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "ith = udf(ith_, DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46e28546-8702-4933-8fdd-5ff5ceb7b4af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Logistic Regression\n",
    "\n",
    "lr3 = LogisticRegression(featuresCol = 'features', labelCol = 'isFraud', maxIter=10)\n",
    "lrModel3 = lr3.fit(train_df_feature.select(\"isFraud\",\"features\"))\n",
    "predictions = lrModel3.transform(test_df_feature)\n",
    "probability_col_lr3 = predictions.select(\"TransactionID\",ith(\"probability\", lit(1)))\n",
    "pd_lr3 = probability_col_lr3.toPandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab7db80e-734e-4de5-9ace-655a12119d9a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Display the TransactionID and Probability column\n",
    "display(pd_lr3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aeacddb6-a0da-4246-91f8-5adf4da213f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Gradient Boosted Trees\n",
    "\n",
    "gbt3 = GBTClassifier(labelCol=\"isFraud\", featuresCol=\"features\", maxIter = 5)\n",
    "gbtModel3 = gbt3.fit(train_df_feature.select(\"isFraud\",\"features\"))\n",
    "\n",
    "predictions = gbtModel3.transform(test_df_feature)\n",
    "probability_col_gbt3 = predictions.select(\"TransactionID\",ith(\"probability\", lit(1)))\n",
    "pd_gbt3 = probability_col_gbt3.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afb0af7a-6d9b-4926-b1ea-3908ae244238",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Display the TransactionID and Probability column\n",
    "display(pd_gbt3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4eaa060d-fe96-4fbe-ad6b-88cfc3d92554",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Preparing Train Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a34666b-b977-4cf5-9b16-3de29b394868",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Reference Link: https://medium.com/@junwan01/oversampling-and-undersampling-with-pyspark-5dbc25cdf253\n",
    "#Using sampleBy function to create a undersampled dataset for cross validator\n",
    "#Using these ratios, the number of samples of both classes will be equal in train_df_subset\n",
    "\n",
    "fractions = {1: 1.0, 0: 0.036}\n",
    "train_df_subset = train_df_feature.sampleBy(\"isFraud\", fractions=fractions, seed=40)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "499a2a7b-4160-4936-bfad-ff4a16344486",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Caching the subset\n",
    "train_df_subset = train_df_subset.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0ce18e92-82d8-4837-85c7-40211121bb5c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## ML PART 4: Applying TrainValidationSplit to the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91e37858-2974-4447-bd1a-0d30b5f1dffc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Reference Link: https://spark.apache.org/docs/latest/ml-tuning.html\n",
    "#An estimator, evaluator and param grid are initialized. They are fed to the TrainValidationSplit function. A unique seed is provided for reproducible results.\n",
    "#This function then chooses the best parameters for the logistic regression model and transforms the test dataset.\n",
    "\n",
    "lr_tv = LogisticRegression(featuresCol = 'features', labelCol = 'isFraud', maxIter = 10)\n",
    "pipeline_tv = Pipeline(stages=[lr_tv])\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr_tv.regParam, [0, 0.01, 0.1]) \\\n",
    "    .addGrid(lr_tv.elasticNetParam, [0, 0.3, 0.5])\\\n",
    "    .build()\n",
    "\n",
    "trainval = TrainValidationSplit(estimator=pipeline_tv,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(labelCol = 'isFraud'),\n",
    "                          seed = -8807458389991577450)\n",
    "\n",
    "trainvalModel = trainval.fit(train_df_subset.select('features','isFraud'))\n",
    "\n",
    "predictions = trainvalModel.transform(test_df_feature)\n",
    "probability_col_lr_tv = predictions.select('TransactionID',ith(\"probability\", lit(1)))\n",
    "\n",
    "pd_lr_tv = probability_col_lr_tv.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3f7276f-93dd-487c-b8be-5f82b9a5aa37",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Display the TransactionID and Probability column\n",
    "display(pd_lr_tv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "885f38a3-b03c-4bb7-a453-d2e5d683349c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Reference Link: https://spark.apache.org/docs/latest/ml-tuning.html\n",
    "#An estimator, evaluator and param grid are initialized. They are fed to the TrainValidationSplit function. A unique seed is provided for reproducible results.\n",
    "#This function then chooses the best parameters for the gradient boosted tree model and transforms the test dataset.\n",
    "\n",
    "\n",
    "gbt_tv = GBTClassifier(featuresCol = 'features', labelCol = 'isFraud', maxIter = 5)\n",
    "pipeline_tv = Pipeline(stages=[gbt_tv])\n",
    "\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "             .addGrid(gbt_tv.maxDepth, [3, 5, 7])\\\n",
    "             .build()\n",
    "trainval = TrainValidationSplit(estimator=pipeline_tv,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(labelCol = 'isFraud'),\n",
    "                          seed = -8807458389991577450)\n",
    "\n",
    "trainvalModel = trainval.fit(train_df_subset.select('features','isFraud'))\n",
    "\n",
    "predictions = trainvalModel.transform(test_df_feature)\n",
    "probability_col_gbt_tv = predictions.select('TransactionID',ith(\"probability\", lit(1)))\n",
    "\n",
    "pd_gbt_tv = probability_col_gbt_tv.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c84d0b2-b040-475f-ac2e-5f6b157bf6df",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Display the TransactionID and Probability column\n",
    "display(pd_gbt_tv)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Team 21 - Final",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
